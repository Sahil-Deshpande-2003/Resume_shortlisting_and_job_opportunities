{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126e0b15-58f2-47a5-82ac-ad485712d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import PyPDF2\n",
    "import re  \n",
    "# Import the re module for regular expressions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2f1787-5a96-4efe-af1a-27660bd10d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_resume(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            text = ''\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9789a05-c3ac-4e7e-88b9-8d0f26601c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF resume file\n",
    "pdf_resume_path = 'aman_resume'\n",
    "\n",
    "# Read the PDF resume\n",
    "resume_text = read_pdf_resume(pdf_resume_path)\n",
    "\n",
    "# Print the extracted text\n",
    "# print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84291174-182d-4039-a723-b35fef24337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!! IGNORE THIS  !!!!!!!!!!!\n",
    "# def preprocess_resume(text):\n",
    "#     # Preprocessing to remove non-technical sections\n",
    "#     # Example: Removing non-technical sections such as \"Personal Summary\", \"Education\", etc.\n",
    "#     # You can customize this based on the structure of your resume files\n",
    "#     # For simplicity, let's assume non-technical sections start with \"Personal Summary:\" and end with \"Education:\"\n",
    "#     match = re.search(r'(?<=Technical Skills:).*?(?=Education:)', text, re.IGNORECASE|re.DOTALL)\n",
    "#     if match:\n",
    "#         technical_text = match.group(0)\n",
    "#         return technical_text\n",
    "#     else:\n",
    "#         return \"\"  # Return an empty string if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd94127a-2484-4fa9-abdf-28ca5dba142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the resume to extract technical information\n",
    "# technical_text = preprocess_resume(resume_text)\n",
    "# print(technical_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad24764-9388-4258-b8e7-e774456a93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign documents\n",
    "# d0 = 'Geeks for data structures and Algorithm geeks aman and aslam C++, C, sayyad'\n",
    "# d1 = 'Geeks for DP table, in MySQL java aliya aslam sayyad'\n",
    "# d2 = 'r2j the nasim aslam sayyad'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "# string = [d0, d1, d2]\n",
    "string = [resume_text]\n",
    "# print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45fd99f-6517-4b34-b2e0-3adae3e7eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf7ce58-75e0-42b9-a887-930380e834bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " # get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c584be3e-028c-4fda-8f34-7f332c705320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "20 : 1.0\n",
      "2019 : 1.0\n",
      "2021 : 1.0\n",
      "2022 : 1.0\n",
      "2023 : 1.0\n",
      "2023competitions : 1.0\n",
      "2023github : 1.0\n",
      "2023position : 1.0\n",
      "2025 : 1.0\n",
      "23 : 1.0\n",
      "24 : 1.0\n",
      "2d : 1.0\n",
      "67 : 1.0\n",
      "84 : 1.0\n",
      "87 : 1.0\n",
      "89 : 1.0\n",
      "92 : 1.0\n",
      "9673632003 : 1.0\n",
      "97 : 1.0\n",
      "99 : 1.0\n",
      "able : 1.0\n",
      "ac : 1.0\n",
      "academic : 1.0\n",
      "access : 1.0\n",
      "activities : 1.0\n",
      "actuator : 1.0\n",
      "adcs : 1.0\n",
      "address : 1.0\n",
      "algebra : 1.0\n",
      "algorithm : 1.0\n",
      "algorithms : 1.0\n",
      "aman : 1.0\n",
      "analyze : 1.0\n",
      "android : 1.0\n",
      "app : 1.0\n",
      "approach : 1.0\n",
      "apr : 1.0\n",
      "areas : 1.0\n",
      "arithmetic : 1.0\n",
      "array : 1.0\n",
      "asci : 1.0\n",
      "aslam : 1.0\n",
      "aspect : 1.0\n",
      "aspects : 1.0\n",
      "attitude : 1.0\n",
      "aug : 1.0\n",
      "axis : 1.0\n",
      "balaji : 1.0\n",
      "based : 1.0\n",
      "bc : 1.0\n",
      "binary : 1.0\n",
      "board : 1.0\n",
      "build : 1.0\n",
      "built : 1.0\n",
      "calculator : 1.0\n",
      "carefully : 1.0\n",
      "cases : 1.0\n",
      "cbse : 1.0\n",
      "cgpa : 1.0\n",
      "class : 1.0\n",
      "club : 1.0\n",
      "code : 1.0\n",
      "coding : 1.0\n",
      "coep : 1.0\n",
      "coeptech : 1.0\n",
      "college : 1.0\n",
      "collegemaharashtra : 1.0\n",
      "comp : 1.0\n",
      "competition : 1.0\n",
      "complexity : 1.0\n",
      "computation : 1.0\n",
      "computations : 1.0\n",
      "computer : 1.0\n",
      "conducted : 1.0\n",
      "consisting : 1.0\n",
      "control : 1.0\n",
      "coordinator : 1.0\n",
      "course : 1.0\n",
      "create : 1.0\n",
      "creating : 1.0\n",
      "crucial : 1.0\n",
      "csat : 1.0\n",
      "csi : 1.0\n",
      "curricular : 1.0\n",
      "data : 1.0\n",
      "date : 1.0\n",
      "dec : 1.0\n",
      "details : 1.0\n",
      "determination : 1.0\n",
      "develop : 1.0\n",
      "developed : 1.0\n",
      "developer : 1.0\n",
      "development : 1.0\n",
      "devised : 1.0\n",
      "different : 1.0\n",
      "distribution : 1.0\n",
      "distributors : 1.0\n",
      "dp : 1.0\n",
      "drawing : 1.0\n",
      "duration : 1.0\n",
      "dynamic : 1.0\n",
      "education : 1.0\n",
      "effecient : 1.0\n",
      "effectively : 1.0\n",
      "effectiveness : 1.0\n",
      "electivesdata : 1.0\n",
      "email : 1.0\n",
      "enable : 1.0\n",
      "engineering : 1.0\n",
      "english : 1.0\n",
      "enhance : 1.0\n",
      "ensure : 1.0\n",
      "entails : 1.0\n",
      "event : 1.0\n",
      "exam : 1.0\n",
      "extensive : 1.0\n",
      "featured : 1.0\n",
      "feb : 1.0\n",
      "focused : 1.0\n",
      "github : 1.0\n",
      "githubobject : 1.0\n",
      "given : 1.0\n",
      "good : 1.0\n",
      "hackerrank : 1.0\n",
      "handles : 1.0\n",
      "high : 1.0\n",
      "higher : 1.0\n",
      "hindi : 1.0\n",
      "hosted : 1.0\n",
      "hour : 1.0\n",
      "identify : 1.0\n",
      "implement : 1.0\n",
      "implemented : 1.0\n",
      "improvement : 1.0\n",
      "initiative : 1.0\n",
      "insertion : 1.0\n",
      "institute : 1.0\n",
      "introduces : 1.0\n",
      "isro : 1.0\n",
      "issue : 1.0\n",
      "jan : 1.0\n",
      "javamysql : 1.0\n",
      "jee : 1.0\n",
      "jun : 1.0\n",
      "junior : 1.0\n",
      "junkie : 1.0\n",
      "kabaddi : 1.0\n",
      "known : 1.0\n",
      "lab : 1.0\n",
      "language : 1.0\n",
      "larger : 1.0\n",
      "level : 1.0\n",
      "like : 1.0\n",
      "linear : 1.0\n",
      "link : 1.0\n",
      "linked : 1.0\n",
      "linkprojects : 1.0\n",
      "list : 1.0\n",
      "live : 1.0\n",
      "logic : 1.0\n",
      "logical : 1.0\n",
      "madhyamik : 1.0\n",
      "mains : 1.0\n",
      "make : 1.0\n",
      "makefile : 1.0\n",
      "maneuvering : 1.0\n",
      "mar : 1.0\n",
      "marathilanguages : 1.0\n",
      "match : 1.0\n",
      "matches : 1.0\n",
      "medical : 1.0\n",
      "medicines : 1.0\n",
      "member : 1.0\n",
      "memberextra : 1.0\n",
      "mhtcet : 1.0\n",
      "microprocessor : 1.0\n",
      "mobile : 1.0\n",
      "msbshse : 1.0\n",
      "national : 1.0\n",
      "necessary : 1.0\n",
      "nlogn : 1.0\n",
      "numbers : 1.0\n",
      "object : 1.0\n",
      "oop : 1.0\n",
      "operator : 1.0\n",
      "orbit : 1.0\n",
      "order : 1.0\n",
      "organized : 1.0\n",
      "orientation : 1.0\n",
      "oriented : 1.0\n",
      "outcomes : 1.0\n",
      "page : 1.0\n",
      "participants : 1.0\n",
      "passed : 1.0\n",
      "past : 1.0\n",
      "percentile : 1.0\n",
      "perform : 1.0\n",
      "pitch : 1.0\n",
      "place : 1.0\n",
      "practice : 1.0\n",
      "precise : 1.0\n",
      "precision : 1.0\n",
      "present : 1.0\n",
      "presented : 1.0\n",
      "primary : 1.0\n",
      "problem : 1.0\n",
      "problems : 1.0\n",
      "proficiency : 1.0\n",
      "programming : 1.0\n",
      "project : 1.0\n",
      "prototyping : 1.0\n",
      "question : 1.0\n",
      "questions : 1.0\n",
      "rapid : 1.0\n",
      "reaction : 1.0\n",
      "real : 1.0\n",
      "reasoning : 1.0\n",
      "recursion : 1.0\n",
      "recycler : 1.0\n",
      "reduce : 1.0\n",
      "refining : 1.0\n",
      "repository : 1.0\n",
      "responsibility : 1.0\n",
      "responsible : 1.0\n",
      "results : 1.0\n",
      "retracer : 1.0\n",
      "ridesharing : 1.0\n",
      "role : 1.0\n",
      "round : 1.0\n",
      "rounds : 1.0\n",
      "routes : 1.0\n",
      "satellite : 1.0\n",
      "saturation : 1.0\n",
      "sayyad : 1.0\n",
      "sayyadaa21 : 1.0\n",
      "schedule : 1.0\n",
      "school : 1.0\n",
      "score : 1.0\n",
      "scores : 1.0\n",
      "second : 1.0\n",
      "secondary : 1.0\n",
      "segment : 1.0\n",
      "sep : 1.0\n",
      "serves : 1.0\n",
      "shri : 1.0\n",
      "simulate : 1.0\n",
      "simulation : 1.0\n",
      "simulations : 1.0\n",
      "single : 1.0\n",
      "small : 1.0\n",
      "software : 1.0\n",
      "solve : 1.0\n",
      "specific : 1.0\n",
      "specifically : 1.0\n",
      "sports : 1.0\n",
      "stack : 1.0\n",
      "state : 1.0\n",
      "statement : 1.0\n",
      "stores : 1.0\n",
      "streamline : 1.0\n",
      "structure : 1.0\n",
      "structures : 1.0\n",
      "students : 1.0\n",
      "studying : 1.0\n",
      "subjects : 1.0\n",
      "table : 1.0\n",
      "tackle : 1.0\n",
      "takes : 1.0\n",
      "task : 1.0\n",
      "tech : 1.0\n",
      "technical : 1.0\n",
      "technique : 1.0\n",
      "techniques : 1.0\n",
      "test : 1.0\n",
      "tested : 1.0\n",
      "theory : 1.0\n",
      "time : 1.0\n",
      "timeline : 1.0\n",
      "tree : 1.0\n",
      "unique : 1.0\n",
      "university : 1.0\n",
      "upcoming : 1.0\n",
      "update : 1.0\n",
      "updates : 1.0\n",
      "used : 1.0\n",
      "using : 1.0\n",
      "utilizes : 1.0\n",
      "validate : 1.0\n",
      "various : 1.0\n",
      "vidyalaya : 1.0\n",
      "view : 1.0\n",
      "volleyball : 1.0\n",
      "wheel : 1.0\n",
      "xii : 1.0\n",
      "year : 1.0\n",
      "zest : 1.0\n"
     ]
    }
   ],
   "source": [
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfbd0e3d-4cc3-437e-bfad-c8c53e36b6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "# print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92dc6f18-2007-48cb-9433-f95decf79984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf-idf value:\n",
      "  (0, 147)\t0.03641785203646149\n",
      "  (0, 167)\t0.03641785203646149\n",
      "  (0, 127)\t0.03641785203646149\n",
      "  (0, 109)\t0.03641785203646149\n",
      "  (0, 24)\t0.03641785203646149\n",
      "  (0, 83)\t0.03641785203646149\n",
      "  (0, 173)\t0.03641785203646149\n",
      "  (0, 40)\t0.03641785203646149\n",
      "  (0, 172)\t0.03641785203646149\n",
      "  (0, 82)\t0.03641785203646149\n",
      "  (0, 291)\t0.03641785203646149\n",
      "  (0, 146)\t0.03641785203646149\n",
      "  (0, 98)\t0.03641785203646149\n",
      "  (0, 15)\t0.03641785203646149\n",
      "  (0, 18)\t0.03641785203646149\n",
      "  (0, 10)\t0.03641785203646149\n",
      "  (0, 162)\t0.03641785203646149\n",
      "  (0, 142)\t0.03641785203646149\n",
      "  (0, 195)\t0.07283570407292297\n",
      "  (0, 13)\t0.03641785203646149\n",
      "  (0, 19)\t0.03641785203646149\n",
      "  (0, 0)\t0.03641785203646149\n",
      "  (0, 242)\t0.03641785203646149\n",
      "  (0, 174)\t0.03641785203646149\n",
      "  (0, 114)\t0.03641785203646149\n",
      "  :\t:\n",
      "  (0, 294)\t0.03641785203646149\n",
      "  (0, 237)\t0.07283570407292297\n",
      "  (0, 280)\t0.03641785203646149\n",
      "  (0, 51)\t0.07283570407292297\n",
      "  (0, 65)\t0.03641785203646149\n",
      "  (0, 136)\t0.03641785203646149\n",
      "  (0, 87)\t0.03641785203646149\n",
      "  (0, 22)\t0.03641785203646149\n",
      "  (0, 8)\t0.03641785203646149\n",
      "  (0, 269)\t0.03641785203646149\n",
      "  (0, 108)\t0.03641785203646149\n",
      "  (0, 72)\t0.03641785203646149\n",
      "  (0, 233)\t0.03641785203646149\n",
      "  (0, 41)\t0.03641785203646149\n",
      "  (0, 31)\t0.03641785203646149\n",
      "  (0, 12)\t0.03641785203646149\n",
      "  (0, 58)\t0.03641785203646149\n",
      "  (0, 17)\t0.03641785203646149\n",
      "  (0, 176)\t0.03641785203646149\n",
      "  (0, 21)\t0.03641785203646149\n",
      "  (0, 64)\t0.03641785203646149\n",
      "  (0, 67)\t0.03641785203646149\n",
      "  (0, 234)\t0.03641785203646149\n",
      "  (0, 106)\t0.03641785203646149\n",
      "  (0, 77)\t0.07283570407292297\n"
     ]
    }
   ],
   "source": [
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fdf3a85-c472-44d2-9a43-b7fca058892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.03641785 0.03641785 0.10925356 0.0728357  0.21850711 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.14567141\n",
      "  0.03641785 0.03641785 0.03641785 0.0728357  0.10925356 0.03641785\n",
      "  0.0728357  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.0728357  0.0728357  0.0728357  0.03641785 0.03641785\n",
      "  0.0728357  0.03641785 0.03641785 0.03641785 0.03641785 0.0728357\n",
      "  0.10925356 0.0728357  0.0728357  0.21850711 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.14567141 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.10925356 0.03641785 0.0728357\n",
      "  0.03641785 0.03641785 0.03641785 0.10925356 0.03641785 0.03641785\n",
      "  0.14567141 0.03641785 0.0728357  0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.0728357  0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.0728357  0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.10925356 0.0728357  0.0728357\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.0728357\n",
      "  0.03641785 0.0728357  0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.0728357  0.03641785 0.0728357  0.10925356\n",
      "  0.03641785 0.10925356 0.03641785 0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.0728357  0.10925356 0.03641785 0.03641785\n",
      "  0.03641785 0.0728357  0.10925356 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.0728357  0.03641785\n",
      "  0.03641785 0.03641785 0.0728357  0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.14567141 0.0728357  0.03641785 0.18208926 0.0728357\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.0728357  0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.10925356\n",
      "  0.03641785 0.03641785 0.03641785 0.0728357  0.0728357  0.03641785\n",
      "  0.0728357  0.03641785 0.03641785 0.25492496 0.0728357  0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.0728357  0.10925356 0.03641785\n",
      "  0.0728357  0.0728357  0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.0728357  0.0728357\n",
      "  0.03641785 0.03641785 0.03641785 0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.10925356 0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.10925356 0.03641785 0.03641785\n",
      "  0.0728357  0.03641785 0.0728357  0.03641785 0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.0728357  0.0728357  0.03641785 0.03641785\n",
      "  0.03641785 0.03641785 0.03641785 0.03641785 0.0728357  0.03641785\n",
      "  0.03641785 0.0728357 ]]\n"
     ]
    }
   ],
   "source": [
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1c8d96d-da50-4637-83ee-2409dd4692c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import csv\\nimport nltk\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\n\\n\\n# Download NLTK data (if not already downloaded)\\n# nltk.download('punkt')\\n# nltk.download('stopwords')\\n# nltk.download('wordnet')\\n\\n# Get English stopwords\\nstop_words = set(stopwords.words('english'))\\n\\n\\n# Initialize WordNetLemmatizer\\nlemmatizer = WordNetLemmatizer()\\n\\n# Create an empty list to store filtered and lemmatized tokens\\nfiltered_lemmatized_tokens_list = []\\n\\n# Specify the path to your CSV file\\ncsv_file_path = 'skill2vec_1K.csv'\\n\\n# Create an empty list to store filtered tokens\\nfiltered_tokens_list = []\\n\\n# Open the CSV file in read mode\\nwith open(csv_file_path, mode='r') as file:\\n    # Create a CSV reader object\\n    csv_reader = csv.reader(file)\\n\\n    # Iterate over each row in the CSV file\\n    for row in csv_reader:\\n        # Tokenize the text in each non-empty column of the row\\n        for text in row:\\n            if text:  # Check if text is not empty\\n                \\n                # tokens = word_tokenize(text)\\n\\n                 # Replace commas with whitespace\\n                text = text.replace(',', ' ').replace('_', ' ').replace('(', ' ').replace(')', ' ')\\n                \\n                # Tokenize text based on whitespace and commas\\n                tokens = text.split()  # Split using whitespace by default\\n                \\n                # Remove specific characters like '/', '-'\\n                tokens = [re.sub(r'[\\\\/\\\\-]', '', token) for token in tokens]\\n\\n                # Filter out empty tokens\\n                tokens = [token for token in tokens if token]\\n                \\n                # Remove stop words and numbers\\n                # filtered_tokens = [token for token in tokens if token.lower() not in stop_words and not token.isdigit()]\\n\\n                filtered_lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens \\n                                               if token.lower() not in stop_words and not token.isdigit()]\\n                \\n                # Append filtered tokens to the list\\n                if filtered_lemmatized_tokens:\\n                    # filtered_tokens_list.append(filtered_tokens)\\n                    # filtered_tokens_list.extend(filtered_tokens)\\n\\n                \\n                    # Extend the list with filtered and lemmatized tokens\\n                    filtered_lemmatized_tokens_list.extend(filtered_lemmatized_tokens)\\n                    \\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!!! IGNORE THIS !!! #\n",
    "# ''' import csv\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# # Download NLTK data (if not already downloaded)\n",
    "# # nltk.download('punkt')\n",
    "# # nltk.download('stopwords')\n",
    "# # nltk.download('wordnet')\n",
    "\n",
    "# # Get English stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# # Initialize WordNetLemmatizer\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# # Create an empty list to store filtered and lemmatized tokens\n",
    "# filtered_lemmatized_tokens_list = []\n",
    "\n",
    "# # Specify the path to your CSV file\n",
    "# csv_file_path = 'skill2vec_1K.csv'\n",
    "\n",
    "# # Create an empty list to store filtered tokens\n",
    "# filtered_tokens_list = []\n",
    "\n",
    "# # Open the CSV file in read mode\n",
    "# with open(csv_file_path, mode='r') as file:\n",
    "#     # Create a CSV reader object\n",
    "#     csv_reader = csv.reader(file)\n",
    "\n",
    "#     # Iterate over each row in the CSV file\n",
    "#     for row in csv_reader:\n",
    "#         # Tokenize the text in each non-empty column of the row\n",
    "#         for text in row:\n",
    "#             if text:  # Check if text is not empty\n",
    "                \n",
    "#                 # tokens = word_tokenize(text)\n",
    "\n",
    "#                  # Replace commas with whitespace\n",
    "#                 text = text.replace(',', ' ').replace('_', ' ').replace('(', ' ').replace(')', ' ')\n",
    "                \n",
    "#                 # Tokenize text based on whitespace and commas\n",
    "#                 tokens = text.split()  # Split using whitespace by default\n",
    "                \n",
    "#                 # Remove specific characters like '/', '-'\n",
    "#                 tokens = [re.sub(r'[\\/\\-]', '', token) for token in tokens]\n",
    "\n",
    "#                 # Filter out empty tokens\n",
    "#                 tokens = [token for token in tokens if token]\n",
    "                \n",
    "#                 # Remove stop words and numbers\n",
    "#                 # filtered_tokens = [token for token in tokens if token.lower() not in stop_words and not token.isdigit()]\n",
    "\n",
    "#                 filtered_lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens \n",
    "#                                                if token.lower() not in stop_words and not token.isdigit()]\n",
    "                \n",
    "#                 # Append filtered tokens to the list\n",
    "#                 if filtered_lemmatized_tokens:\n",
    "#                     # filtered_tokens_list.append(filtered_tokens)\n",
    "#                     # filtered_tokens_list.extend(filtered_tokens)\n",
    "\n",
    "                \n",
    "#                     # Extend the list with filtered and lemmatized tokens\n",
    "#                     filtered_lemmatized_tokens_list.extend(filtered_lemmatized_tokens)\n",
    "                    \n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0af8a8-fe72-49dd-8dcc-488bded238d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in filtered_lemmatized_tokens_list:\n",
    "#     print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8827f56-8cc8-4ef9-aef1-5246acc934e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install --upgrade tensorflow\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Define the neural network model\n",
    "# model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(10,)), # Adjust input_shape according to your word embeddings\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the modele to train a Word2Vec model directly on your set of technical words, you may need to experiment with different settings, such as the dimensionality of the embeddings (vector_size), the wi\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Print the model summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a2a4b0-fedc-4386-9e63-e351800daff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I have to do\n",
    "# - create the neural network from skill words. If input word output to this model will be 0 or 1 i.e is it tech skill word or non-tech skill word\n",
    "# - After that I will find the vector correspond document consisting filtered words then use cosine similarity or KNN\n",
    "# - or else I can use word embedding for some other application like do classification or find the similarity.\n",
    "\n",
    "# TODO: properly understand how neural network is working \n",
    "# Before Tuesday it should be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b67b4e22-3629-4402-9792-b1e7a1c25a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IGNORE THIS !!! #\n",
    "# ''' #!pip install gensim\n",
    "# import numpy as np\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Example dataset (replace with your own preprocessed dataset)\n",
    "# # dataset = [['this', 'is', 'sentence', 'one'],\n",
    "# #            ['this', 'is', 'sentence', 'two'],\n",
    "# #            ['another', 'example', 'sentence']]\n",
    "\n",
    "# # College-related words\n",
    "# college_words = ['College', 'University', 'Institute', 'Institution', 'Academia', 'Campus', 'Degree', 'Diploma', \n",
    "#                  'Major', 'Minor', 'Bachelor\\'s', 'Master\\'s', 'Doctorate', 'Graduation', 'Commencement', \n",
    "#                  'Registrar', 'Admissions', 'Tuition', 'Fees', 'Scholarships']\n",
    "\n",
    "# # Academic Information words\n",
    "# academic_words = ['CGPA', 'GPA', 'Course', 'Class', 'Lecture', 'Seminar', 'Workshop', 'Syllabus', 'Curriculum', \n",
    "#                   'Assignment', 'Project', 'Thesis', 'Dissertation', 'Examination', 'Test', 'Quiz']\n",
    "\n",
    "# # Contact Information words\n",
    "# contact_words = ['Mobile', 'Phone', 'Telephone', 'Cellphone', 'Email', 'Address', 'Postal Address', 'Zip Code', \n",
    "#                  'Postcode', 'Fax']\n",
    "\n",
    "# # Personal Names words\n",
    "# name_words = ['First Name', 'Last Name', 'Full Name', 'Middle Name', 'Surname', 'Given Name', 'Family Name', \n",
    "#               'Nickname', 'Initials']\n",
    "\n",
    "# # Language-related words\n",
    "# language_words = ['Language', 'English', 'Spanish', 'French', 'German', 'Chinese', 'Japanese', 'Italian', \n",
    "#                   'Russian', 'Arabic', 'Portuguese', 'Hindi', 'Bengali', 'Urdu', 'Tamil', 'Telugu', 'Korean', \n",
    "#                   'Greek', 'Latin', 'Sanskrit', 'Vocabulary', 'Grammar', 'Syntax', 'Pronunciation', 'Accent', \n",
    "#                   'Dialect', 'Linguistics', 'Translation', 'Interpreter', 'Bilingual', 'Multilingual']\n",
    "\n",
    "# # Combine all arrays into a single array\n",
    "# combined_array = college_words + academic_words + contact_words + name_words + language_words\n",
    "\n",
    "# filtered_lemmatized_tokens1 = [lemmatizer.lemmatize(token.lower()) for token in  combined_array\n",
    "#                                                if token.lower() not in stop_words and not token.isdigit()]\n",
    "\n",
    "\n",
    "# filtered_lemmatized_tokens_list1 = []\n",
    "\n",
    "# # Append filtered tokens to the list\n",
    "# if filtered_lemmatized_tokens1:\n",
    "#     # filtered_tokens_list.append(filtered_tokens)\n",
    "                \n",
    "#     # Extend the list with filtered and lemmatized tokens\n",
    "#     filtered_lemmatized_tokens_list1.extend(filtered_lemmatized_tokens)\n",
    "                    \n",
    "# # print(len(combined_array))\n",
    "# dataset = [filtered_lemmatized_tokens_list + filtered_lemmatized_tokens_list1]\n",
    "\n",
    "# # print(len(dataset[0]))\n",
    "# X_train = []\n",
    "\n",
    "# # print(dataset)\n",
    "# # Train Word2Vec model\n",
    "# model = Word2Vec(dataset, vector_size=64, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Get word embeddings\n",
    "# word_embeddings = model.wv\n",
    "\n",
    "# vocabulary = word_embeddings.key_to_index\n",
    "# # print(vocabulary)\n",
    "# # Get embedding for a specific word\n",
    "# for i in dataset[0]:\n",
    "#     X_train.append(word_embeddings[i])\n",
    "# # embedding = word_embeddings['rest']\n",
    "# print(embedding)\n",
    "\n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array([1]*1609 + [0]*172)\n",
    "# # print(len(X))\n",
    "# # print(y.size)\n",
    "\n",
    "# # Assuming X_train and y_train are your feature and target arrays\n",
    "# # Shuffle the dataset randomly\n",
    "# indices = np.arange(len(X_train))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# # Extract samples belonging to each class\n",
    "# class_0_indices = indices[y_train == 0]\n",
    "# class_1_indices = indices[y_train == 1]\n",
    "\n",
    "# # Take a subset of the majority class samples equal to the number of minority class samples\n",
    "# subset_indices = np.random.choice(class_1_indices, size=len(class_0_indices), replace=False)\n",
    "\n",
    "# # Combine subset with all samples from the minority class\n",
    "# balanced_indices = np.concatenate([class_0_indices, subset_indices])\n",
    "\n",
    "# # Shuffle the combined dataset again\n",
    "# np.random.shuffle(balanced_indices)\n",
    "\n",
    "# # Use the balanced indices to get the balanced dataset\n",
    "# X_train_balanced = X_train[balanced_indices]\n",
    "# y_train_balanced = y_train[balanced_indices]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=0.2, random_state=42)\n",
    "# # print(X_train[1])\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62baf718-778d-48c8-81ce-bf93f449bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IGNORE THIS !!! #\n",
    "# '''\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Define the neural network model\n",
    "# model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(64,)),  # Adjust embedding_dim according to your word embeddings\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80773a9f-a5ff-4274-9eb6-e78824e442b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IGNORE THIS !!! #\n",
    "# '''\n",
    "# word = ['sahhil']\n",
    "# dataset = [word]\n",
    "\n",
    "# # Train Word2Vec model\n",
    "# model1 = Word2Vec(dataset, vector_size=64, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Get word embeddings\n",
    "# word_embeddings = model1.wv\n",
    "\n",
    "# word_vector = word_embeddings['sahhil']\n",
    "\n",
    "# word_vector = word_vector.reshape(1, -1) \n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(word_vector)\n",
    "\n",
    "# # Interpret the results\n",
    "# print(\"Predictions:\", predictions)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa1a9b-8a5f-4365-a680-0f0407f8dfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bda975-3b73-48aa-ade8-765b08d2eaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
