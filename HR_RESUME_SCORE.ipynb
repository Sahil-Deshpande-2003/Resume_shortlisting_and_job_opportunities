{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126e0b15-58f2-47a5-82ac-ad485712d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import PyPDF2\n",
    "import re  \n",
    "# Import the re module for regular expressions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2f1787-5a96-4efe-af1a-27660bd10d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_resume(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PdfReader(file)\n",
    "            text = ''\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9789a05-c3ac-4e7e-88b9-8d0f26601c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF resume file\n",
    "pdf_resume_path = 'aman_resume'\n",
    "\n",
    "# Read the PDF resume\n",
    "resume_text = read_pdf_resume(pdf_resume_path)\n",
    "\n",
    "# Print the extracted text\n",
    "# print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84291174-182d-4039-a723-b35fef24337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!! IGNORE THIS  !!!!!!!!!!!\n",
    "# def preprocess_resume(text):\n",
    "#     # Preprocessing to remove non-technical sections\n",
    "#     # Example: Removing non-technical sections such as \"Personal Summary\", \"Education\", etc.\n",
    "#     # You can customize this based on the structure of your resume files\n",
    "#     # For simplicity, let's assume non-technical sections start with \"Personal Summary:\" and end with \"Education:\"\n",
    "#     match = re.search(r'(?<=Technical Skills:).*?(?=Education:)', text, re.IGNORECASE|re.DOTALL)\n",
    "#     if match:\n",
    "#         technical_text = match.group(0)\n",
    "#         return technical_text\n",
    "#     else:\n",
    "#         return \"\"  # Return an empty string if no match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd94127a-2484-4fa9-abdf-28ca5dba142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the resume to extract technical information\n",
    "# technical_text = preprocess_resume(resume_text)\n",
    "# print(technical_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad24764-9388-4258-b8e7-e774456a93ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign documents\n",
    "# d0 = 'Geeks for data structures and Algorithm geeks aman and aslam C++, C, sayyad'\n",
    "# d1 = 'Geeks for DP table, in MySQL java aliya aslam sayyad'\n",
    "# d2 = 'r2j the nasim aslam sayyad'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "# string = [d0, d1, d2]\n",
    "string = [resume_text]\n",
    "# print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d45fd99f-6517-4b34-b2e0-3adae3e7eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "# tfidf = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf7ce58-75e0-42b9-a887-930380e834bc",
   "metadata": {},
   "outputs": [],
   "source": [
    " # get tf-df values\n",
    "# result = tfidf.fit_transform(string)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c584be3e-028c-4fda-8f34-7f332c705320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get idf values\n",
    "# print('\\nidf values:')\n",
    "# for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "#     print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfbd0e3d-4cc3-437e-bfad-c8c53e36b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indexing\n",
    "# print('\\nWord indexes:')\n",
    "# print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92dc6f18-2007-48cb-9433-f95decf79984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # display tf-idf values\n",
    "# print('\\ntf-idf value:')\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fdf3a85-c472-44d2-9a43-b7fca058892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in matrix form\n",
    "# print('\\ntf-idf values in matrix form:')\n",
    "# print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1c8d96d-da50-4637-83ee-2409dd4692c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!!! IGNORE THIS !!! #\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Initialize WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create an empty list to store filtered and lemmatized tokens\n",
    "filtered_lemmatized_tokens_list = []\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'skill2vec_1K.csv'\n",
    "\n",
    "# Create an empty list to store filtered tokens\n",
    "filtered_tokens_list = []\n",
    "\n",
    "# Open the CSV file in read mode\n",
    "with open(csv_file_path, mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "    csv_reader = csv.reader(file)\n",
    "\n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        # Tokenize the text in each non-empty column of the row\n",
    "        for text in row:\n",
    "            if text:  # Check if text is not empty\n",
    "                \n",
    "                # tokens = word_tokenize(text)\n",
    "\n",
    "                 # Replace commas with whitespace\n",
    "                text = text.replace(',', ' ').replace('_', ' ').replace('(', ' ').replace(')', ' ')\n",
    "                \n",
    "                # Tokenize text based on whitespace and commas\n",
    "                tokens = text.split()  # Split using whitespace by default\n",
    "                \n",
    "                # Remove specific characters like '/', '-'\n",
    "                tokens = [re.sub(r'[\\/\\-]', '', token) for token in tokens]\n",
    "\n",
    "                # Filter out empty tokens\n",
    "                tokens = [token for token in tokens if token]\n",
    "                \n",
    "                # Remove stop words and numbers\n",
    "                # filtered_tokens = [token for token in tokens if token.lower() not in stop_words and not token.isdigit()]\n",
    "\n",
    "                filtered_lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens \n",
    "                                               if token.lower() not in stop_words and not token.isdigit()]\n",
    "                \n",
    "                # Append filtered tokens to the list\n",
    "                if filtered_lemmatized_tokens:\n",
    "                    # filtered_tokens_list.append(filtered_tokens)\n",
    "                    # filtered_tokens_list.extend(filtered_tokens)\n",
    "\n",
    "                \n",
    "                    # Extend the list with filtered and lemmatized tokens\n",
    "                    filtered_lemmatized_tokens_list.extend(filtered_lemmatized_tokens)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf0af8a8-fe72-49dd-8dcc-488bded238d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in filtered_lemmatized_tokens_list:\n",
    "#     print(i, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8827f56-8cc8-4ef9-aef1-5246acc934e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills extracted from resume: ['table', 'structure', 'programming', 'algorithm', 'time', 'operator', 'problem', 'project', 'data', 'stack', 'high', 'order', 'problem', 'distribution', 'project', 'build', 'software', 'developer', 'control', 'problem', 'improvement', 'control', 'system', 'event', 'real', 'time', 'access', 'coding', 'problem', 'coding', 'first', 'programming', 'passed', 'good', 'test']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# print(nlp)\n",
    "\n",
    "# Sample resume text (replace this with your actual resume text)\n",
    "# resume_text = \"\"\"\n",
    "# Experienced software engineer proficient in Python, Java, and JavaScript. Skilled in web development, database management, and cloud computing. Strong problem-solving skills and ability to work in a team environment c++.\n",
    "# \"\"\"\n",
    "\n",
    "# Define list of skills\n",
    "skills_list = filtered_lemmatized_tokens_list\n",
    "\n",
    "# Initialize PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp(skill) for skill in skills_list]\n",
    "# print(patterns)\n",
    "matcher.add(\"SKILL\", None, *patterns)\n",
    "\n",
    "# Process the resume text\n",
    "doc = nlp(resume_text)\n",
    "\n",
    "# Extract skills\n",
    "skills = []\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    skills.append(doc[start:end].text)\n",
    "\n",
    "print(\"Skills extracted from resume:\", skills)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "725f3275-61e2-4a4a-a939-f31a11acdb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resume_Text = ' '.join(skills)\n",
    "string = [Resume_Text]\n",
    "# print(Resum_Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7911a8bf-7501-4d28-bb2c-e66f768b7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c21aa14f-dbf7-4a3d-92ad-832569304483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 23)\t0.13483997249264842\n",
      "  (0, 9)\t0.13483997249264842\n",
      "  (0, 14)\t0.13483997249264842\n",
      "  (0, 3)\t0.26967994498529685\n",
      "  (0, 0)\t0.13483997249264842\n",
      "  (0, 18)\t0.13483997249264842\n",
      "  (0, 8)\t0.13483997249264842\n",
      "  (0, 11)\t0.13483997249264842\n",
      "  (0, 4)\t0.26967994498529685\n",
      "  (0, 6)\t0.13483997249264842\n",
      "  (0, 19)\t0.13483997249264842\n",
      "  (0, 2)\t0.13483997249264842\n",
      "  (0, 7)\t0.13483997249264842\n",
      "  (0, 13)\t0.13483997249264842\n",
      "  (0, 10)\t0.13483997249264842\n",
      "  (0, 20)\t0.13483997249264842\n",
      "  (0, 5)\t0.13483997249264842\n",
      "  (0, 17)\t0.26967994498529685\n",
      "  (0, 15)\t0.5393598899705937\n",
      "  (0, 12)\t0.13483997249264842\n",
      "  (0, 24)\t0.26967994498529685\n",
      "  (0, 1)\t0.13483997249264842\n",
      "  (0, 16)\t0.26967994498529685\n",
      "  (0, 21)\t0.13483997249264842\n",
      "  (0, 22)\t0.13483997249264842\n"
     ]
    }
   ],
   "source": [
    " # get tf-df values\n",
    "result = tfidf.fit_transform(string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8b61752-6cf4-4528-b065-520bad5f7eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "access : 1.0\n",
      "algorithm : 1.0\n",
      "build : 1.0\n",
      "coding : 1.0\n",
      "control : 1.0\n",
      "data : 1.0\n",
      "developer : 1.0\n",
      "distribution : 1.0\n",
      "event : 1.0\n",
      "good : 1.0\n",
      "high : 1.0\n",
      "improvement : 1.0\n",
      "operator : 1.0\n",
      "order : 1.0\n",
      "passed : 1.0\n",
      "problem : 1.0\n",
      "programming : 1.0\n",
      "project : 1.0\n",
      "real : 1.0\n",
      "software : 1.0\n",
      "stack : 1.0\n",
      "structure : 1.0\n",
      "table : 1.0\n",
      "test : 1.0\n",
      "time : 1.0\n"
     ]
    }
   ],
   "source": [
    " # get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "    print(ele1, ':', ele2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3619eb4e-172c-42ff-abbf-f69e847a9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'table': 22, 'structure': 21, 'programming': 16, 'algorithm': 1, 'time': 24, 'operator': 12, 'problem': 15, 'project': 17, 'data': 5, 'stack': 20, 'high': 10, 'order': 13, 'distribution': 7, 'build': 2, 'software': 19, 'developer': 6, 'control': 4, 'improvement': 11, 'event': 8, 'real': 18, 'access': 0, 'coding': 3, 'passed': 14, 'good': 9, 'test': 23}\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2a2a4b0-fedc-4386-9e63-e351800daff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what I have to do\n",
    "# - create the neural network from skill words. If input word output to this model will be 0 or 1 i.e is it tech skill word or non-tech skill word\n",
    "# - After that I will find the vector correspond document consisting filtered words then use cosine similarity or KNN\n",
    "# - or else I can use word embedding for some other application like do classification or find the similarity.\n",
    "\n",
    "# TODO: properly understand how neural network is working \n",
    "# Before Tuesday it should be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b67b4e22-3629-4402-9792-b1e7a1c25a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IGNORE THIS !!! #\n",
    "# ''' #!pip install gensim\n",
    "# import numpy as np\n",
    "# from gensim.models import Word2Vec\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Example dataset (replace with your own preprocessed dataset)\n",
    "# # dataset = [['this', 'is', 'sentence', 'one'],\n",
    "# #            ['this', 'is', 'sentence', 'two'],\n",
    "# #            ['another', 'example', 'sentence']]\n",
    "\n",
    "# # College-related words\n",
    "# college_words = ['College', 'University', 'Institute', 'Institution', 'Academia', 'Campus', 'Degree', 'Diploma', \n",
    "#                  'Major', 'Minor', 'Bachelor\\'s', 'Master\\'s', 'Doctorate', 'Graduation', 'Commencement', \n",
    "#                  'Registrar', 'Admissions', 'Tuition', 'Fees', 'Scholarships']\n",
    "\n",
    "# # Academic Information words\n",
    "# academic_words = ['CGPA', 'GPA', 'Course', 'Class', 'Lecture', 'Seminar', 'Workshop', 'Syllabus', 'Curriculum', \n",
    "#                   'Assignment', 'Project', 'Thesis', 'Dissertation', 'Examination', 'Test', 'Quiz']\n",
    "\n",
    "# # Contact Information words\n",
    "# contact_words = ['Mobile', 'Phone', 'Telephone', 'Cellphone', 'Email', 'Address', 'Postal Address', 'Zip Code', \n",
    "#                  'Postcode', 'Fax']\n",
    "\n",
    "# # Personal Names words\n",
    "# name_words = ['First Name', 'Last Name', 'Full Name', 'Middle Name', 'Surname', 'Given Name', 'Family Name', \n",
    "#               'Nickname', 'Initials']\n",
    "\n",
    "# # Language-related words\n",
    "# language_words = ['Language', 'English', 'Spanish', 'French', 'German', 'Chinese', 'Japanese', 'Italian', \n",
    "#                   'Russian', 'Arabic', 'Portuguese', 'Hindi', 'Bengali', 'Urdu', 'Tamil', 'Telugu', 'Korean', \n",
    "#                   'Greek', 'Latin', 'Sanskrit', 'Vocabulary', 'Grammar', 'Syntax', 'Pronunciation', 'Accent', \n",
    "#                   'Dialect', 'Linguistics', 'Translation', 'Interpreter', 'Bilingual', 'Multilingual']\n",
    "\n",
    "# # Combine all arrays into a single array\n",
    "# combined_array = college_words + academic_words + contact_words + name_words + language_words\n",
    "\n",
    "# filtered_lemmatized_tokens1 = [lemmatizer.lemmatize(token.lower()) for token in  combined_array\n",
    "#                                                if token.lower() not in stop_words and not token.isdigit()]\n",
    "\n",
    "\n",
    "# filtered_lemmatized_tokens_list1 = []\n",
    "\n",
    "# # Append filtered tokens to the list\n",
    "# if filtered_lemmatized_tokens1:\n",
    "#     # filtered_tokens_list.append(filtered_tokens)\n",
    "                \n",
    "#     # Extend the list with filtered and lemmatized tokens\n",
    "#     filtered_lemmatized_tokens_list1.extend(filtered_lemmatized_tokens)\n",
    "                    \n",
    "# # print(len(combined_array))\n",
    "# dataset = [filtered_lemmatized_tokens_list + filtered_lemmatized_tokens_list1]\n",
    "\n",
    "# # print(len(dataset[0]))\n",
    "# X_train = []\n",
    "\n",
    "# # print(dataset)\n",
    "# # Train Word2Vec model\n",
    "# model = Word2Vec(dataset, vector_size=64, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Get word embeddings\n",
    "# word_embeddings = model.wv\n",
    "\n",
    "# vocabulary = word_embeddings.key_to_index\n",
    "# # print(vocabulary)\n",
    "# # Get embedding for a specific word\n",
    "# for i in dataset[0]:\n",
    "#     X_train.append(word_embeddings[i])\n",
    "# # embedding = word_embeddings['rest']\n",
    "# print(embedding)\n",
    "\n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array([1]*1609 + [0]*172)\n",
    "# # print(len(X))\n",
    "# # print(y.size)\n",
    "\n",
    "# # Assuming X_train and y_train are your feature and target arrays\n",
    "# # Shuffle the dataset randomly\n",
    "# indices = np.arange(len(X_train))\n",
    "# np.random.shuffle(indices)\n",
    "\n",
    "# # Extract samples belonging to each class\n",
    "# class_0_indices = indices[y_train == 0]\n",
    "# class_1_indices = indices[y_train == 1]\n",
    "\n",
    "# # Take a subset of the majority class samples equal to the number of minority class samples\n",
    "# subset_indices = np.random.choice(class_1_indices, size=len(class_0_indices), replace=False)\n",
    "\n",
    "# # Combine subset with all samples from the minority class\n",
    "# balanced_indices = np.concatenate([class_0_indices, subset_indices])\n",
    "\n",
    "# # Shuffle the combined dataset again\n",
    "# np.random.shuffle(balanced_indices)\n",
    "\n",
    "# # Use the balanced indices to get the balanced dataset\n",
    "# X_train_balanced = X_train[balanced_indices]\n",
    "# y_train_balanced = y_train[balanced_indices]\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train_balanced, y_train_balanced, test_size=0.2, random_state=42)\n",
    "# # print(X_train[1])\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62baf718-778d-48c8-81ce-bf93f449bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IGNORE THIS !!! #\n",
    "# '''\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Define the neural network model\n",
    "# model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(64,)),  # Adjust embedding_dim according to your word embeddings\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "# print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80773a9f-a5ff-4274-9eb6-e78824e442b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! IGNORE THIS !!! #\n",
    "# '''\n",
    "# word = ['sahhil']\n",
    "# dataset = [word]\n",
    "\n",
    "# # Train Word2Vec model\n",
    "# model1 = Word2Vec(dataset, vector_size=64, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Get word embeddings\n",
    "# word_embeddings = model1.wv\n",
    "\n",
    "# word_vector = word_embeddings['sahhil']\n",
    "\n",
    "# word_vector = word_vector.reshape(1, -1) \n",
    "\n",
    "# # Make predictions\n",
    "# predictions = model.predict(word_vector)\n",
    "\n",
    "# # Interpret the results\n",
    "# print(\"Predictions:\", predictions)\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa1a9b-8a5f-4365-a680-0f0407f8dfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bda975-3b73-48aa-ade8-765b08d2eaab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
